{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFWkHbRfS6mcMNXs+k59Ka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuxiufeng/Spam_ham_Classification/blob/main/Text%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip files"
      ],
      "metadata": {
        "id": "8L8B6jr5IyLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip enron1_test.zip\n",
        "!unzip enron1_train.zip\n",
        "!unzip enron4_test.zip\n",
        "!unzip enron4_train.zip\n",
        "!unzip enron1_test.zip\n",
        "!unzip enron1_train.zip\n",
        "!unzip hw1_test.zip\n",
        "!unzip hw1_train.zip\n"
      ],
      "metadata": {
        "id": "lfIqbafV9oKF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Words"
      ],
      "metadata": {
        "id": "PcpZknSALn80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
        "              \"you're\", \"you've\",\"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
        "              'yourselves', 'he', 'him', 'his','himself', 'she', \"she's\", 'her', \n",
        "              'hers', 'herself', 'it', \"it's\", 'its', 'itself','they', 'them', \n",
        "              'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',\n",
        "              'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', \n",
        "              'was', 'were', 'be','been', 'being', 'have', 'has', 'had', 'having', \n",
        "              'do', 'does', 'did', 'doing', 'a','an', 'the', 'and', 'but', 'if', \n",
        "              'or', 'because', 'as', 'until', 'while', 'of', 'at','by', 'for', \n",
        "              'with', 'about', 'against', 'between', 'into', 'through', 'during',\n",
        "              'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', \n",
        "              'out', 'on','off', 'over', 'under', 'again', 'further', 'then', 'once', \n",
        "              'here', 'there', 'when','where', 'why', 'how', 'all', 'any', 'both', \n",
        "              'each', 'few', 'more', 'most', 'other','some', 'such', 'no', 'nor', 'not', \n",
        "              'only', 'own', 'same', 'so', 'than', 'too', 'very','s', 't', 'can', 'will', \n",
        "              'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd','ll', 'm', 'o', 're', \n",
        "              've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", \n",
        "              'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
        "              'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
        "              'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won',\n",
        "              \"won't\", 'wouldn', \"wouldn't\"]"
      ],
      "metadata": {
        "id": "iKNCrQbReRFU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library"
      ],
      "metadata": {
        "id": "ZN34fl0ILsvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import codecs\n",
        "import numpy as np\n",
        "from decimal import Decimal\n",
        "from math import log10 as log\n",
        "from collections import Counter\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "Q8ZJ_vkYci1k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "lXBIW0SOMDIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  path: The path of folder. For example, 'content/enron1'\n",
        "*  True_or_False_train_set: True means it is train set; otherwise, it is test set\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ivqBbRxYNfFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def import_data_train_or_test(path, True_or_False_train_set):\n",
        "    ham_raw_content = []\n",
        "    spam_raw_content = []\n",
        "    all_spam_ham_content = \"\"\n",
        "    if True_or_False_train_set == True:\n",
        "        path = path + '/train'\n",
        "    else:\n",
        "        path = path + '/test'\n",
        "    path_ham = path + '/ham/'\n",
        "    path_spam = path + '/spam/'\n",
        "    spam_raw_contents_list = os.listdir(path_spam)\n",
        "    ham_raw_contents_list = os.listdir(path_ham)\n",
        "    for spam_raw_contents in spam_raw_contents_list:\n",
        "      total_spam_path = path_spam + spam_raw_contents\n",
        "      spam_raw_content.append(open(total_spam_path, \"r\", errors = 'ignore').read())\n",
        "      all_spam_ham_content = all_spam_ham_content + \" \" + open(total_spam_path, \"r\", errors = 'ignore').read()\n",
        "    for ham_raw_contents in ham_raw_contents_list:\n",
        "      total_ham_path = path_ham + ham_raw_contents\n",
        "      ham_raw_content.append(open(total_ham_path, \"r\", errors = 'ignore').read())\n",
        "      all_spam_ham_content = all_spam_ham_content + \" \" + open(total_ham_path, \"r\", errors = 'ignore').read()\n",
        "    # we find the size of the dataset and the number of instances with spam and number of instances with ham\n",
        "    size_of_whole_dataset = len(ham_raw_contents_list) + len(spam_raw_contents_list)\n",
        "    size_of_ham_dataset = len(ham_raw_contents_list)\n",
        "    size_of_spam_dataset = len(spam_raw_contents_list)\n",
        "    return spam_raw_content, ham_raw_content, all_spam_ham_content, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset"
      ],
      "metadata": {
        "id": "xgbWpg_cQdOY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_raw_content, ham_raw_content, all_spam_ham_content, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset = import_data_train_or_test('/content/enron1', True)"
      ],
      "metadata": {
        "id": "2N-OSRsGbOqQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_of_whole_dataset"
      ],
      "metadata": {
        "id": "r7OBA5Rl62kg",
        "outputId": "c9eaa579-a115-431b-aafe-c2d02b31a728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "450"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words model"
      ],
      "metadata": {
        "id": "1Vi1s5pbgFql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UGpTiB3u9kmF"
      },
      "outputs": [],
      "source": [
        "def bag_of_words_model(dataset_name, True_or_False_train_set):\n",
        "    \"\"\"\n",
        "    This function returns the bag of words for given dataset\n",
        "    :param dataset_name: This is the dataset name\n",
        "    :param True_or_False_train_set: In this if it is true then we have train data else test data\n",
        "    :return: We return the bag for words representation for spam and ham files\n",
        "    \"\"\"\n",
        "    spam_raw_content, ham_raw_content, all_spam_ham_content, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset = import_data_train_or_test(dataset_name, True_or_False_train_set)\n",
        "    total_file_dictionary = {}\n",
        "    total_file_data = re.findall(\"[a-zA-Z]+\", all_spam_ham_content)\n",
        "    text_frequency_in_all_folders = {}\n",
        "    # at first we find all the words in the given dataset and find the frequencies in the whole dataset\n",
        "    for each_word in total_file_data:\n",
        "        each_word = each_word.lower()\n",
        "        if each_word in total_file_dictionary:\n",
        "            continue\n",
        "        else:\n",
        "            # I got the list of stem words from the nltk library\n",
        "            if each_word not in stop_words:\n",
        "                total_file_dictionary[each_word] = 0\n",
        "        if each_word in text_frequency_in_all_folders:\n",
        "            if each_word not in stop_words:\n",
        "                text_frequency_in_all_folders[each_word] = text_frequency_in_all_folders[each_word] + 1\n",
        "        else:\n",
        "            if each_word not in stop_words:\n",
        "                text_frequency_in_all_folders[each_word] = 1\n",
        "    # In the following steps we find the words in the spam dataset and create the bag of words\n",
        "    spam_email_bag_of_words = []\n",
        "    spam_mail_in_all_documents = {}\n",
        "    # The frequencies of each words are stored in the case of bag of words.\n",
        "    for each_spam_mail in spam_raw_content:\n",
        "        temp_dict = copy.deepcopy(total_file_dictionary)\n",
        "        each_spam_mail1 = re.findall(\"[a-zA-Z]+\", each_spam_mail)\n",
        "        # Here we create the bag of words for each document and append it in a list\n",
        "        for each_word in each_spam_mail1:\n",
        "            each_word = each_word.lower()\n",
        "            if each_word in temp_dict:\n",
        "                temp_dict[each_word] = temp_dict[each_word] + 1\n",
        "        # Here we store all the words in the spam dataset\n",
        "        spam_mail_in_all_documents = Counter(spam_mail_in_all_documents) + Counter(temp_dict)\n",
        "        spam_email_bag_of_words.append(temp_dict)\n",
        "    # In the following steps we find all the ham words and add them in the bag of words\n",
        "    ham_mail_in_all_documents = {}\n",
        "    ham_email_bag_of_words = []\n",
        "    for each_ham_mail in ham_raw_content:\n",
        "        # Here we create the bag of words for each document and append it in a list\n",
        "        temp_dict = copy.deepcopy(total_file_dictionary)\n",
        "        each_ham_mail1 = re.findall(\"[a-zA-Z]+\", each_ham_mail)\n",
        "        for each_word in each_ham_mail1:\n",
        "            each_word = each_word.lower()\n",
        "            if each_word in temp_dict:\n",
        "                temp_dict[each_word] = temp_dict[each_word] + 1\n",
        "        # Here we store all the words in the ham dataset\n",
        "        ham_mail_in_all_documents = Counter(ham_mail_in_all_documents) + Counter(temp_dict)\n",
        "        ham_email_bag_of_words.append(temp_dict)\n",
        "    return spam_email_bag_of_words, ham_email_bag_of_words, text_frequency_in_all_folders, spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spam_email_bag_of_words, ham_email_bag_of_words, text_frequency_in_all_folders, spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary = bag_of_words_model('/content/enron1', True)"
      ],
      "metadata": {
        "id": "W6me6TpLboUQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_email_bag_of_words"
      ],
      "metadata": {
        "id": "XaNmz1tLfJJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bernoulli model"
      ],
      "metadata": {
        "id": "HmGJau1zgHmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_bernoulli_model(dataset_name, True_or_False_train_set):\n",
        "    \"\"\"\n",
        "    This function returns the bernoulli model for given dataset\n",
        "    :param dataset_name: This is the dataset name\n",
        "    :param True_or_False_train_set: In this if it is true then we have train data else test data\n",
        "    :return: We return the bernoulli model representation for spam and ham files\n",
        "    \"\"\"\n",
        "    spam_raw_content, ham_raw_content, all_spam_ham_content, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset = import_data_train_or_test(\n",
        "        dataset_name, True_or_False_train_set)\n",
        "    total_file_dictionary = {}\n",
        "    total_file_data = re.findall(\"[a-zA-Z]+\", all_spam_ham_content)\n",
        "    # at first we find all the words in the given dataset and find the occurrences in the whole dataset\n",
        "    for each_word in total_file_data:\n",
        "        # The words are converted to their lower case forms\n",
        "        each_word = each_word.lower()\n",
        "        if each_word in total_file_dictionary:\n",
        "            continue\n",
        "        else:\n",
        "            if each_word not in stop_words:\n",
        "                total_file_dictionary[each_word] = 0\n",
        "    # In the following steps we find the words in the spam dataset and create the model\n",
        "    spam_email_bernoulli_model = []\n",
        "    spam_mail_in_all_documents = {}\n",
        "    for each_spam_mail in spam_raw_content:\n",
        "        # Here we create the bag of words for each document and append it in a list\n",
        "        temp_dict = copy.deepcopy(total_file_dictionary)\n",
        "        each_spam_mail1 = re.findall(\"[a-zA-Z]+\", each_spam_mail)\n",
        "        for each_word in each_spam_mail1:\n",
        "            each_word = each_word.lower()\n",
        "            if each_word in temp_dict:\n",
        "                temp_dict[each_word] = 1\n",
        "                # Here we store all the words in the ham dataset\n",
        "                spam_mail_in_all_documents[each_word] = 1\n",
        "        temp_list = list(temp_dict.values())\n",
        "        spam_email_bernoulli_model.append(temp_dict)\n",
        "    # In the following steps we find the words in the ham dataset and create the model\n",
        "    ham_email_bernoulli_model = []\n",
        "    ham_mail_in_all_documents = {}\n",
        "    for each_ham_mail in ham_raw_content:\n",
        "        # Here we create the bag of words for each document and append it in a list\n",
        "        temp_dict = copy.deepcopy(total_file_dictionary)\n",
        "        each_ham_mail1 = re.findall(\"[a-zA-Z]+\", each_ham_mail)\n",
        "        for each_word in each_ham_mail1:\n",
        "            each_word = each_word.lower()\n",
        "            if each_word in temp_dict:\n",
        "                temp_dict[each_word] = 1\n",
        "                ham_mail_in_all_documents[each_word] = 1\n",
        "        ham_email_bernoulli_model.append(temp_dict)\n",
        "    return spam_email_bernoulli_model, ham_email_bernoulli_model, spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary"
      ],
      "metadata": {
        "id": "T6_J3BD8gLBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_email_bernoulli_model, ham_email_bernoulli_model, spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary= convert_to_bernoulli_model('/content/enron1', True)"
      ],
      "metadata": {
        "id": "yHvtMTQJhCXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "fhcZ762yjlPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def train_multinomial_NB(spam_email_bag_of_words, ham_email_bag_of_words, text_frequency_in_all_folders,\n",
        "                         spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset,\n",
        "                         size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary):\n",
        "    \"\"\"\n",
        "    This is the main algorithm to train the multinomial Naive Bayes\n",
        "    :param total_file_dictionary: This is the list containing all the words in the training examples\n",
        "    :param spam_email_bag_of_words:  This is the list of all words in each spam document (1st value is for first document and so on)\n",
        "    :param ham_email_bag_of_words: This is the list of all words in each ham document (1st value is for first document and so on)\n",
        "    :param text_frequency_in_all_folders: This is the total text in all documents with their frequencies\n",
        "    :param spam_mail_in_all_documents: This is the total text in all spam documents with their frequencies\n",
        "    :param ham_mail_in_all_documents: This is the total text in all ham documents with their frequencies\n",
        "    :param size_of_whole_dataset: This is total number of files in all dataset\n",
        "    :param size_of_spam_dataset: This is total number of files in all spam dataset\n",
        "    :param size_of_ham_dataset: This is total number of files in all ham dataset\n",
        "    :return: prior and conditional probability for both spam and ham(all these values are in log)\n",
        "    \"\"\"\n",
        "    no_of_docs = size_of_whole_dataset\n",
        "    # We will first do it for the spam\n",
        "    no_of_spam_docs = size_of_spam_dataset\n",
        "    # We create the variables to store the values\n",
        "    prior = {}\n",
        "    conditional_probability = {}\n",
        "    conditional_probability[\"spam\"] = {}\n",
        "    conditional_probability[\"ham\"] = {}\n",
        "    conditional_probability_of_non_occurring_word = {}\n",
        "    conditional_probability_of_non_occurring_word[\"spam\"] = {}\n",
        "    conditional_probability_of_non_occurring_word[\"ham\"] = {}\n",
        "    value = Decimal(no_of_spam_docs / float(no_of_docs))\n",
        "    # First we calculate the priors for the spam and ham dataset\n",
        "    prior[\"spam\"] = log(value)\n",
        "    no_of_ham_docs = size_of_ham_dataset\n",
        "    total_number_of_words_in_ham = sum(ham_mail_in_all_documents.values())\n",
        "    prior[\"ham\"] = log(no_of_ham_docs / float(no_of_docs))\n",
        "    total_number_of_words_in_spam = sum(ham_mail_in_all_documents.values())\n",
        "    # Now we calculate the values for the conditional probabilities\n",
        "    for each_word in list(spam_mail_in_all_documents):\n",
        "        conditional_probability[\"spam\"][each_word] = log((spam_mail_in_all_documents[each_word] + 1) / (\n",
        "            float(total_number_of_words_in_spam + len(text_frequency_in_all_folders))))\n",
        "\n",
        "    # Now we will do the same procedure for ham docs\n",
        "    for each_word in list(ham_mail_in_all_documents):\n",
        "        conditional_probability[\"ham\"][each_word] = log((ham_mail_in_all_documents[each_word] + 1) / (\n",
        "            float(total_number_of_words_in_ham + len(text_frequency_in_all_folders))))\n",
        "    # These are the values for the conditional probabilities whose words are not in the training dataset\n",
        "    conditional_probability_of_non_occurring_word[\"ham\"] = log(\n",
        "        1 / (float(total_number_of_words_in_ham + len(text_frequency_in_all_folders))))\n",
        "    conditional_probability_of_non_occurring_word[\"spam\"] = log(\n",
        "        1 / (float(total_number_of_words_in_spam + len(text_frequency_in_all_folders))))\n",
        "    return prior, conditional_probability, conditional_probability_of_non_occurring_word"
      ],
      "metadata": {
        "id": "Nue1byXwh_pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multinomial_naive_bayes(prior, conditional_probability, conditional_probability_of_non_occurring_word,\n",
        "                                 an_email_bag_of_words_test):\n",
        "    \"\"\"\n",
        "    :param conditional_probability_of_non_occurring_word: This is the conditional probability for each word in the testing set which is not in the training data\n",
        "    :param prior: This is the prior for all classes\n",
        "    :param conditional_probability:  This is the conditional probability for each word in vocabulary in spam and ham data\n",
        "    :param an_email_bag_of_words_test: This is the given test instance we want to classify\n",
        "    :return: the class of the given email\n",
        "    \"\"\"\n",
        "    score = {}\n",
        "    for each_class in list(prior):\n",
        "        score[each_class] = prior[each_class]\n",
        "        for each_word in list(an_email_bag_of_words_test):\n",
        "            if an_email_bag_of_words_test[each_word] != 0:\n",
        "                try:\n",
        "                    score[each_class] += conditional_probability[each_class][each_word]\n",
        "                # This is the case if the word was not in the train data and thus the laplace pruning gives this result\n",
        "                except KeyError:\n",
        "                    score[each_class] += conditional_probability_of_non_occurring_word[each_class]\n",
        "    # Here we are taking spam as 1 and ham as -1\n",
        "    if score[\"spam\"] > score[\"ham\"]:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "lny0qN0DiVgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prior, conditional_probability, conditional_probability_of_non_occurring_word = train_multinomial_NB(spam_email_bag_of_words, ham_email_bag_of_words, text_frequency_in_all_folders,\n",
        "                         spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset,\n",
        "                         size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary)"
      ],
      "metadata": {
        "id": "c2sl2xrNir9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prior"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kotci3eCjyah",
        "outputId": "4f4ede46-5305-44bb-8d59-1ebfaf0cd1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'spam': -0.5359412181195794, 'ham': -0.14942183071816256}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(true_y, predicted_y):\n",
        "    \"\"\"\n",
        "    This function is used to calculate the accuracy\n",
        "    :param true_y: These are the given values of class variable\n",
        "    :param predicted_y: These are the predicted values of the class variable\n",
        "    :return: the accuracy\n",
        "    \"\"\"\n",
        "    accuracy_count = 0\n",
        "    for each in range(len(true_y)):\n",
        "        if true_y[each] == predicted_y[each]:\n",
        "            accuracy_count = accuracy_count + 1\n",
        "    return accuracy_count / float(len(true_y))\n",
        "\n",
        "\n",
        "def precision(true_y, predicted_y):\n",
        "    \"\"\"\n",
        "    This function is used to calculate the precision\n",
        "    :param true_y: These are the given values of class variable\n",
        "    :param predicted_y: These are the predicted values of the class variable\n",
        "    :return: the precision\n",
        "    \"\"\"\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    for each in range(len(true_y)):\n",
        "        if true_y[each] == predicted_y[each] and predicted_y[each] == 1:\n",
        "            true_positives += 1\n",
        "        if true_y[each] != predicted_y[each] and predicted_y[each] == 1:\n",
        "            false_positives += 1\n",
        "    return true_positives / float(true_positives + false_positives)\n",
        "\n",
        "\n",
        "def recall(true_y, predicted_y):\n",
        "    \"\"\"\n",
        "    This function is used to calculate the recall\n",
        "    :param true_y: These are the given values of class variable\n",
        "    :param predicted_y: These are the predicted values of the class variable\n",
        "    :return: the recall\n",
        "    \"\"\"\n",
        "    true_positives = 0\n",
        "    false_negetives = 0\n",
        "    for each in range(len(true_y)):\n",
        "        if true_y[each] == predicted_y[each] and predicted_y[each] == 1:\n",
        "            true_positives += 1\n",
        "        if true_y[each] != predicted_y[each] and predicted_y[each] == 0:\n",
        "            false_negetives += 1\n",
        "    return true_positives / float(true_positives + false_negetives)\n",
        "\n",
        "\n",
        "def f1_score(recall, precision):\n",
        "    \"\"\"\n",
        "    This function is used to calculate the f1_score\n",
        "    :param recall: This is the value of recall\n",
        "    :param precision: This is the value of precision\n",
        "    :return: the f1_score\n",
        "    \"\"\"\n",
        "    return (2 * recall * precision) / float(recall + precision)"
      ],
      "metadata": {
        "id": "awjY7Rb38YcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_multinomial_NB(dataset_name):\n",
        "    \"\"\"\n",
        "    This is the method used for evaluation of multinomial NB on a particular dataset\n",
        "    :param dataset_name: This is the given dataset name\n",
        "    :return: The method returns the accuracy, precision, recall and f1_score for the given dataset\n",
        "    \"\"\"\n",
        "    # We first import training data for the training\n",
        "    try:\n",
        "        spam_email_bag_of_words, ham_email_bag_of_words, text_frequency_in_all_folders, spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary = bag_of_words_model(\n",
        "            dataset_name, True)\n",
        "    except:\n",
        "        print (\"You have given wrong file name, please check and run again\")\n",
        "        exit(-1)\n",
        "    prior, conditional_probability, conditional_probability_of_non_occurring_word = train_multinomial_NB(\n",
        "        spam_email_bag_of_words, ham_email_bag_of_words, text_frequency_in_all_folders, spam_mail_in_all_documents,\n",
        "        ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset,\n",
        "        total_file_dictionary)\n",
        "    # We now import the data for testing\n",
        "    spam_email_bag_of_words, ham_email_bag_of_words, text_frequency_in_all_folders, spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary = bag_of_words_model(\n",
        "        dataset_name, False)\n",
        "    # We calculate the evaluation metric\n",
        "    # Here we first predict for the spam class and then the ham class\n",
        "    spam_predict = []\n",
        "    for each_document in spam_email_bag_of_words:\n",
        "        spam_predict.append(test_multinomial_naive_bayes(prior, conditional_probability,\n",
        "                                                                                  conditional_probability_of_non_occurring_word,\n",
        "                                                                                  each_document))\n",
        "    # We  are taking spam as 1\n",
        "    spam_actual = [1] * len(spam_predict)\n",
        "    ham_predict = []\n",
        "    for each_document in ham_email_bag_of_words:\n",
        "        ham_predict.append(test_multinomial_naive_bayes(prior, conditional_probability,\n",
        "                                                                                 conditional_probability_of_non_occurring_word,\n",
        "                                                                                 each_document))\n",
        "    ham_actual = [0] * len(ham_predict)\n",
        "    total_actual = spam_actual + ham_actual\n",
        "    total_predict = spam_predict + ham_predict\n",
        "    # Now we find the evaluation metrics for the method\n",
        "    accur = accuracy(total_actual, total_predict)\n",
        "    prec = precision(total_actual, total_predict)\n",
        "    rec = recall(total_actual, total_predict)\n",
        "    f1 = f1_score(rec, prec)\n",
        "    return accur, prec, rec, f1"
      ],
      "metadata": {
        "id": "wp8MEvLr4Ufw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_multinomial_NB('/content/enron1')"
      ],
      "metadata": {
        "id": "BkOThAE26GEy",
        "outputId": "2172fe04-afc4-454a-ec79-c462e468238c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8114035087719298, 1.0, 0.4228187919463087, 0.5943396226415094)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## discrete Naive Bayes"
      ],
      "metadata": {
        "id": "LkvPhjn-9tOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def discrete_naive_bayes_train(spam_email_bernoulli_model, ham_email_bernoulli_model,\n",
        "                               spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset,\n",
        "                               size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary):\n",
        "    \"\"\"\n",
        "    \n",
        "    :param total_file_dictionary: This is the total words list in the train dataset\n",
        "    :param spam_email_bernoulli_model: This is the list of all the spam documents with bernoulli model\n",
        "    :param ham_email_bernoulli_model: This is the list of all the ham documents with bernoulli model\n",
        "    :param spam_mail_in_all_documents:This is the bernoulli model of all the spam documents\n",
        "    :param ham_mail_in_all_documents:This is the bernoulli model of all the spam documents\n",
        "    :param size_of_whole_dataset: This is total number of files in all dataset\n",
        "    :param size_of_spam_dataset: This is total number of files in all spam dataset\n",
        "    :param size_of_ham_dataset: This is total number of files in all ham dataset\n",
        "    :return: estimate of prior and conditional probability\n",
        "    \"\"\"\n",
        "    no_of_docs = size_of_whole_dataset\n",
        "    no_of_spam_docs = size_of_spam_dataset\n",
        "    no_of_ham_docs = size_of_ham_dataset\n",
        "    prior = {}\n",
        "    # We create variables to store the values\n",
        "    conditional_probability = {}\n",
        "    conditional_probability[\"spam\"] = {}\n",
        "    conditional_probability[\"ham\"] = {}\n",
        "    conditional_probability_of_non_occurring_word = {}\n",
        "    conditional_probability_of_non_occurring_word[\"spam\"] = {}\n",
        "    conditional_probability_of_non_occurring_word[\"ham\"] = {}\n",
        "    # We calculate the prior of both the classes\n",
        "    prior[\"spam\"] = log(no_of_spam_docs / float(no_of_docs))\n",
        "    prior[\"ham\"] = log(no_of_ham_docs / float(no_of_docs))\n",
        "    # We are doing 1-laplace smoothing and thus we add 1 in the numerator and 2 in denominator(since each word can\n",
        "    # have two values o, 1 )\n",
        "    for each_word in spam_mail_in_all_documents:\n",
        "        conditional_probability[\"spam\"][each_word] = log(\n",
        "            1 + spam_mail_in_all_documents[each_word] / (float(no_of_spam_docs + 2)))\n",
        "\n",
        "    for each_word in ham_mail_in_all_documents:\n",
        "        conditional_probability[\"ham\"][each_word] = log(\n",
        "            1 + ham_mail_in_all_documents[each_word] / float(no_of_ham_docs + 2))\n",
        "    # These are the probabilities for the word which are not in the training data and appear in the testing data\n",
        "    conditional_probability_of_non_occurring_word[\"ham\"] = log(1 / (float(no_of_ham_docs + 2)))\n",
        "    conditional_probability_of_non_occurring_word[\"spam\"] = log(1 / (float(no_of_spam_docs + 2)))\n",
        "    return prior, conditional_probability, conditional_probability_of_non_occurring_word"
      ],
      "metadata": {
        "id": "0rDIZlyc4bSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discrete_naive_bayes_test(prior, conditional_probability, conditional_probability_of_non_occurring_word,\n",
        "                              an_email_bag_of_words_test):\n",
        "    \"\"\"\n",
        "    This is the function used to generate the output for the naive bayes algorithm\n",
        "    :param prior: This is the prior generated from the naive bayes\n",
        "    :param conditional_probability: This is the conditional probability generated from the naive bayes\n",
        "    :param conditional_probability_of_non_occurring_word: This is the conditional probability of non occurring word generated from the naive bayes\n",
        "    :param an_email_bag_of_words_test: This is the example on which we are going to test the algo\n",
        "    :return: The class of the given instance\n",
        "    \"\"\"\n",
        "    score = {}\n",
        "    # In the following loop we find the words in the given documents for each class and find the posterior\n",
        "    for each_class in list(prior):\n",
        "        score[each_class] = prior[each_class]\n",
        "        for each_word in list(an_email_bag_of_words_test):\n",
        "            if an_email_bag_of_words_test[each_word] != 0:\n",
        "                try:\n",
        "                    score[each_class] += conditional_probability[each_class][each_word]\n",
        "                # This is the case if the word was not in the train data and thus the laplace pruning gives this result\n",
        "                except KeyError:\n",
        "                    score[each_class] += conditional_probability_of_non_occurring_word[each_class]\n",
        "    # Here we are taking spam as 1 and ham as -1\n",
        "    if score[\"spam\"] > score[\"ham\"]:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "COM5-RvSBWJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_discrete_NB(dataset_name):\n",
        "    \"\"\"\n",
        "    This is the method used for evaluation of multi-nomial NB on a particular dataset\n",
        "    :param dataset_name: This is the given dataset name\n",
        "    :return: We return the accuracy, precision, recall and f1_score for the given dataset\n",
        "    \"\"\"\n",
        "    # We first import training data for the training\n",
        "    try:\n",
        "        spam_email_bag_of_words, ham_email_bag_of_words, spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary = convert_to_bernoulli_model(\n",
        "            dataset_name, True)\n",
        "    except:\n",
        "        print (\"You have given wrong file name, please check and run again\")\n",
        "        exit(-1)\n",
        "    prior, conditional_probability, conditional_probability_of_non_occurring_word = discrete_naive_bayes_train(\n",
        "        spam_email_bag_of_words, ham_email_bag_of_words, spam_mail_in_all_documents,\n",
        "        ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset,\n",
        "        total_file_dictionary)\n",
        "    # We now import the data for testing\n",
        "    spam_email_bag_of_words, ham_email_bag_of_words, spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary = convert_to_bernoulli_model(\n",
        "        dataset_name, False)\n",
        "    # We calculate the evaluation metric\n",
        "    # Here we first predict for the spam class and then the ham class\n",
        "    spam_predict = []\n",
        "    for each_document in spam_email_bag_of_words:\n",
        "        spam_predict.append(discrete_naive_bayes_test(prior, conditional_probability,\n",
        "                                                                           conditional_probability_of_non_occurring_word,\n",
        "                                                                           each_document))\n",
        "    # We  are taking spam as 1\n",
        "    spam_actual = [1] * len(spam_predict)\n",
        "    ham_predict = []\n",
        "    for each_document in ham_email_bag_of_words:\n",
        "        ham_predict.append(discrete_naive_bayes_test(prior, conditional_probability,\n",
        "                                                                          conditional_probability_of_non_occurring_word,\n",
        "                                                                          each_document))\n",
        "    ham_actual = [0] * len(ham_predict)\n",
        "    total_actual = spam_actual + ham_actual\n",
        "    total_predict = spam_predict + ham_predict\n",
        "    # Now we predict the values for all the evaluation metrics\n",
        "    accur = accuracy(total_actual, total_predict)\n",
        "    prec = precision(total_actual, total_predict)\n",
        "    rec = recall(total_actual, total_predict)\n",
        "    f1 = f1_score(rec, prec)\n",
        "    return accur, prec, rec, f1\n",
        "# evaluate_discrete_NB(dataset_name)"
      ],
      "metadata": {
        "id": "vcPSqNxrCL7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_discrete_NB('/content/enron1')"
      ],
      "metadata": {
        "id": "poz3c7x4Cwe9",
        "outputId": "91c9f575-a365-48df-a5fc-8d8088a6da5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9342105263157895,\n",
              " 0.8993288590604027,\n",
              " 0.8993288590604027,\n",
              " 0.8993288590604027)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MCAP Logistic Regression"
      ],
      "metadata": {
        "id": "ohb9c4l_K-cg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vu8Rv5-9KFPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_into_validation_and_train(spam_mail_model, ham_mail_model):\n",
        "    \"\"\"\n",
        "    This is the function used to divide the data into test and train data\n",
        "    :param spam_mail_model: This is the representation(list) of each spam document in the given format\n",
        "    :param ham_mail_model: This is the representation(list) of each ham document in the given format\n",
        "    :return: the train and test set\n",
        "    \"\"\"\n",
        "    # Here spam is 1 and ham is 0 (since we are using sigmoid)\n",
        "    for each_dict in spam_mail_model:\n",
        "        each_dict[\"this_is_the_class_of_the_document\"] = 1\n",
        "        each_dict[\"zero_weight\"] = 1\n",
        "    for each_dict in ham_mail_model:\n",
        "        each_dict[\"this_is_the_class_of_the_document\"] = 0\n",
        "        each_dict[\"zero_weight\"] = 1\n",
        "    all_data = spam_mail_model + ham_mail_model\n",
        "    # We are using this step to shuffle our data so that different data goes into training and testing everything\n",
        "    random.shuffle(all_data)\n",
        "    # 70 percent of the data is for traning and 30 percent of the data is for validation\n",
        "    train_data = all_data[0: int(len(all_data) * .70)]\n",
        "    validation_data = all_data[int(len(all_data) * .70): -1]\n",
        "    return train_data, validation_data"
      ],
      "metadata": {
        "id": "mnj-UMEcKHol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data, validation_data = divide_into_validation_and_train(spam_email_bag_of_words, ham_email_bag_of_words)"
      ],
      "metadata": {
        "id": "xAIbuBPRQnBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_for_class(weights, inputs):\n",
        "    \"\"\"\n",
        "    This function is used to get the output for the given weights and inputs\n",
        "    :param weights: These are the given weights\n",
        "    :param inputs: These are the given inputs\n",
        "    :return: We return the sum of product of individual values of weights and inputs\n",
        "    \"\"\"\n",
        "    value = weights['zero_weight'] * 1\n",
        "    for each in inputs:\n",
        "        if each == 'this_is_the_class_of_the_document' or each == 'zero_weight':\n",
        "            continue\n",
        "        else:\n",
        "            if each in weights and each in inputs:\n",
        "                value = value + (weights[each] * inputs[each])\n",
        "    return value"
      ],
      "metadata": {
        "id": "xfYMOQqdKQ_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_posterior(weights, inputs):\n",
        "    \"\"\"\n",
        "    This function is used to get of the conditional log likelihood\n",
        "    :param weights: These are the given weights\n",
        "    :param inputs: These are the given inputs\n",
        "    :return: We return the sum of product of individual values of weights and inputs\n",
        "    \"\"\"\n",
        "    value = weights['zero_weight'] * 1\n",
        "    for each in inputs:\n",
        "        if each == 'this_is_the_class_of_the_document' or each == 'zero_weight':\n",
        "            continue\n",
        "        else:\n",
        "            if each in weights and each in inputs:\n",
        "                value = value + (weights[each] * inputs[each])\n",
        "    return 1 / (float(1 + np.exp(-value)))"
      ],
      "metadata": {
        "id": "d1A9mB5WKUMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mcap_logistic_regression_train(train_data, total_file_dictionary, eta, lambda_parameter, number_of_iterations):\n",
        "    \"\"\"\n",
        "    This function is used to train the log regression and find the optimum weights for the same\n",
        "    :param number_of_iterations: These are the number of iteration we want to do for the algorithm\n",
        "    :param train_data: This is the train data\n",
        "    :param total_file_dictionary: This is the total list of words in the test data\n",
        "    :param eta: This is the value of eta\n",
        "    :param lambda_parameter: This is the value of lambda used for regularization\n",
        "    :return: We return the optimized weights\n",
        "    \"\"\"\n",
        "    # We are taking w_o outside the array\n",
        "    weights = copy.deepcopy(total_file_dictionary)\n",
        "    for each in weights:\n",
        "        weights[each] = 0\n",
        "    weights['zero_weight'] = 0\n",
        "    # Now we update all the weights\n",
        "    for each in range(number_of_iterations):\n",
        "        for each_instance in train_data:\n",
        "            posterior = get_posterior(weights, each_instance)\n",
        "            sum_of_vals = 0\n",
        "            for each_weight in weights:\n",
        "                # Here I checked if the weight is not equal to 0 or not\n",
        "                if each_instance[each_weight] != 0:\n",
        "                    # This is the case when w_o is used\n",
        "                    if each_weight == \"zero_weight\":\n",
        "                        sum_of_vals = sum_of_vals + eta * (\n",
        "                                each_instance[\"this_is_the_class_of_the_document\"] - posterior)\n",
        "                    else:\n",
        "                        # This is the case when other w's are used\n",
        "                        sum_of_vals = sum_of_vals + eta * (each_instance[each_weight] * (\n",
        "                                each_instance[\"this_is_the_class_of_the_document\"] - posterior))\n",
        "                    weights[each_weight] = weights[each_weight] + sum_of_vals - eta * lambda_parameter * weights[\n",
        "                        each_weight]\n",
        "    return weights"
      ],
      "metadata": {
        "id": "0cJCTjs4KXWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# weights = mcap_logistic_regression_train(train_data, total_file_dictionary, .1, 1, 10)"
      ],
      "metadata": {
        "id": "tMc98xxwRXFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mcap_logistic_regression_test(test_example, weights):\n",
        "    \"\"\"\n",
        "    This function is used to predict the output for the given test_example\n",
        "    :param test_example: This is the given test example\n",
        "    :param weights: These are the given weights\n",
        "    :return: We return the class of the given instance\n",
        "    \"\"\"\n",
        "    value = get_output_for_class(weights, test_example)\n",
        "    # 0 is ham and 1 is spam\n",
        "    if value < 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "wByy4H-bKai1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mcap_validation(train_data, validation_data, total_file_dictionary):\n",
        "    \"\"\"\n",
        "    This function is for getting the best value for the parameter lambda\n",
        "    :param train_data: This is the train data\n",
        "    :param validation_data:  This is the validation data\n",
        "    :param total_file_dictionary:  This is the list of all words.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # Here I am doing the grid search for the lambda parameter\n",
        "    eta = 0.01\n",
        "    max_accuracy = 0\n",
        "    best_lambda_value = 2\n",
        "    # We take the range from 1 increasing 2 at a time\n",
        "    for each_lambda_value in range(1, 8, 2):\n",
        "        # We train the algo with the train data\n",
        "        weights = mcap_logistic_regression_train(train_data, total_file_dictionary, eta, each_lambda_value, 50)\n",
        "        correct_classification = 0\n",
        "        # We test on the validation data\n",
        "        for each_document in validation_data:\n",
        "            output = mcap_logistic_regression_test(each_document, weights)\n",
        "            if output == each_document[\"this_is_the_class_of_the_document\"]:\n",
        "                correct_classification = correct_classification + 1\n",
        "        accuracy = correct_classification / float(len(validation_data))\n",
        "        # Here we get the best lambda value\n",
        "        if accuracy > max_accuracy:\n",
        "            max_accuracy = accuracy\n",
        "            best_lambda_value = each_lambda_value\n",
        "    return best_lambda_value"
      ],
      "metadata": {
        "id": "G5fjQ0U8Kg2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mcap_validation(train_data, validation_data, total_file_dictionary)"
      ],
      "metadata": {
        "id": "_S9AETbbDL7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_MCAP_bag_of_words(dataset_name):\n",
        "    \"\"\"\n",
        "    This is the method used for evaluation of multinomial NB on a particular dataset\n",
        "    :param dataset_name: This is the given dataset name\n",
        "    :return: All the evaluation metrics\n",
        "    \"\"\"\n",
        "    # We first import training data for the training\n",
        "    try:\n",
        "        spam_email_bag_of_words1, ham_email_bag_of_words1, text_frequency_in_all_folders, spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary = bag_of_words_model(\n",
        "            dataset_name, True)\n",
        "    except:\n",
        "        print (\"You have given wrong file name, please check and run again\")\n",
        "        exit(-1)\n",
        "    # Firstly we will divide our training data into training and validation data\n",
        "    train_data, validation_data = divide_into_validation_and_train(spam_email_bag_of_words1,\n",
        "                                                                                            ham_email_bag_of_words1)\n",
        "    # Now we find the lambda value by using the grid search algorithm\n",
        "    lambda_parameter = mcap_validation(train_data, validation_data, total_file_dictionary)\n",
        "    # Here we merge the training data and the validation data again\n",
        "    train_data = train_data + validation_data\n",
        "    alpha_value = 0.01\n",
        "    # In this step the algorithm learns the weights\n",
        "    weights = mcap_logistic_regression_train(train_data, total_file_dictionary, alpha_value,\n",
        "                                                                      lambda_parameter, 100)\n",
        "    # We now import the data for testing\n",
        "    spam_email_bag_of_words_test, ham_email_bag_of_words_test, text_frequency_in_all_folders_test, spam_mail_in_all_documents_test, ham_mail_in_all_documents_test, size_of_whole_dataset_test, size_of_spam_dataset_test, size_of_ham_dataset_test, total_file_dictionary_test = bag_of_words_model(\n",
        "        dataset_name, False)\n",
        "    spam_predict = []\n",
        "    # In this step the algorithm predicts the output for a given dataset\n",
        "    for each_document in spam_email_bag_of_words_test:\n",
        "        spam_predict.append(mcap_logistic_regression_test(each_document, weights))\n",
        "    # We  are taking spam as 1\n",
        "    spam_actual = [1] * len(spam_predict)\n",
        "    ham_predict = []\n",
        "    for each_document in ham_email_bag_of_words_test:\n",
        "        ham_predict.append(mcap_logistic_regression_test(each_document, weights))\n",
        "    ham_actual = [0] * len(ham_predict)\n",
        "    total_actual = spam_actual + ham_actual\n",
        "    total_predict = spam_predict + ham_predict\n",
        "    # Now we find the evaluation metrics for the method\n",
        "    accur = accuracy(total_actual, total_predict)\n",
        "    prec = precision(total_actual, total_predict)\n",
        "    rec = recall(total_actual, total_predict)\n",
        "    f1 = f1_score(rec, prec)\n",
        "    return accur, prec, rec, f1, lambda_parameter\n",
        "    "
      ],
      "metadata": {
        "id": "aa8lu720Tjvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_MCAP_bag_of_words('/content/enron1')"
      ],
      "metadata": {
        "id": "7ZiOmvAnUI61",
        "outputId": "59a87543-d056-4e5a-f734-267c02bac36f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9407894736842105,\n",
              " 0.9485294117647058,\n",
              " 0.8657718120805369,\n",
              " 0.9052631578947368,\n",
              " 3)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_MCAP_bernoulli_model(dataset_name):\n",
        "    \"\"\"\n",
        "    This is the method used for evaluation of multinomial NB on a particular dataset\n",
        "    :param dataset_name: This is the given dataset name\n",
        "    :return: All the evaluation metrics\n",
        "    \"\"\"\n",
        "    # We first import training data for the training\n",
        "    try:\n",
        "        spam_email_bernoulli_model1, ham_email_bernoulli_model1, spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary = convert_to_bernoulli_model(\n",
        "            dataset_name, True)\n",
        "    except:\n",
        "        print (\"You have given wrong file name, please check and run again\")\n",
        "        exit(-1)\n",
        "    # Firstly we will divide our training data into training and validation data\n",
        "    train_data, validation_data = divide_into_validation_and_train(spam_email_bernoulli_model1,\n",
        "                                                                                            ham_email_bernoulli_model1)\n",
        "    # Now we find the lambda value by using the grid search algorithm\n",
        "    lambda_parameter = mcap_validation(train_data, validation_data, total_file_dictionary)\n",
        "    alpha_value = 0.01\n",
        "    # Here we merge the training data and the validation data again\n",
        "    train_data = train_data + validation_data\n",
        "    # In this step the algorithm learns the weights\n",
        "    weights = mcap_logistic_regression_train(train_data, total_file_dictionary, alpha_value,\n",
        "                                                                      lambda_parameter, 100)\n",
        "    # We now import the data for testing\n",
        "    spam_email_bernoulli_model_test, ham_email_bernoulli_model_test, spam_mail_in_all_documents_test, ham_mail_in_all_documents_test, size_of_whole_dataset_test, size_of_spam_dataset_test, size_of_ham_dataset_test, total_file_dictionary_test = convert_to_bernoulli_model(\n",
        "        dataset_name, False)\n",
        "    spam_predict = []\n",
        "    # In this step the algorithm predicts the output for a given dataset\n",
        "    for each_document in spam_email_bernoulli_model_test:\n",
        "        spam_predict.append(mcap_logistic_regression_test(each_document, weights))\n",
        "    # We  are taking spam as 1\n",
        "    spam_actual = [1] * len(spam_predict)\n",
        "    ham_predict = []\n",
        "    for each_document in ham_email_bernoulli_model_test:\n",
        "        ham_predict.append(mcap_logistic_regression_test(each_document, weights))\n",
        "    ham_actual = [0] * len(ham_predict)\n",
        "    total_actual = spam_actual + ham_actual\n",
        "    total_predict = spam_predict + ham_predict\n",
        "    # Now we find the evaluation metrics for the method\n",
        "    accur = accuracy(total_actual, total_predict)\n",
        "    prec = precision(total_actual, total_predict)\n",
        "    rec = recall(total_actual, total_predict)\n",
        "    f1 = f1_score(rec, prec)\n",
        "    return accur, prec, rec, f1, lambda_parameter\n",
        "# evaluate_MCAP_bag_of_words(dataset_name) #for bag of words\n",
        "# evaluate_MCAP_bernoulli_model(dataset_name) # for bernoulli_model"
      ],
      "metadata": {
        "id": "SBuzx9HjTeiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_MCAP_bernoulli_model('/content/enron1')"
      ],
      "metadata": {
        "id": "XyloFmc9YgQI",
        "outputId": "93a93293-aaaa-44a7-8e8b-40d3894ec543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8969298245614035,\n",
              " 0.9811320754716981,\n",
              " 0.697986577181208,\n",
              " 0.8156862745098038,\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGDClassifier"
      ],
      "metadata": {
        "id": "XcfQTJ6xbM-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def parameter_tuning(validation_x, validation_y):\n",
        "    \"\"\"\n",
        "    This function tunes the parameters for the SGDClassifier and returns the classifier with optimized parameters\n",
        "    :param validation_x: This is the validation attribute data\n",
        "    :param validation_y: This is the class values for the given data\n",
        "    :return: The tuned parameter\n",
        "    \"\"\"\n",
        "    parameters_to_be_tuned = {'alpha': (0.01, 0.05),\n",
        "                              'max_iter': (range(500, 3000, 1000)),\n",
        "                              'learning_rate': ('optimal', 'invscaling', 'adaptive'),\n",
        "                              'eta0': (0.3, 0.7),\n",
        "                              'tol': (0.001, 0.005)\n",
        "                              }\n",
        "    SGDclassifier = SGDClassifier()\n",
        "    gridSearch = GridSearchCV(SGDclassifier, parameters_to_be_tuned, cv=5)\n",
        "    gridSearch.fit(validation_x, validation_y)\n",
        "    return gridSearch\n",
        "\n",
        "\n",
        "def train_SGD(train_x, train_y, classifier):\n",
        "    \"\"\"\n",
        "    This is the function used to train the Stochastic Gradient Descent Algorithm\n",
        "    :param train_x: This is train data\n",
        "    :param train_y: This is the train labels/classes\n",
        "    :param Classifier: This is the classifier after parameter tuning\n",
        "    :return: This returns the trained classifier\n",
        "    \"\"\"\n",
        "    return classifier.fit(train_x, train_y)\n",
        "\n",
        "\n",
        "def test_SGD(trained_classifier, test_x, test_y):\n",
        "    \"\"\"\n",
        "    This function is used to test the given classifier\n",
        "    :param trained_classifier: This is the trained classifier we have got after training\n",
        "    :param test_x: This is the test data\n",
        "    :param test_y: These are the test classes\n",
        "    :return: We return the accuracy of the given classifier\n",
        "    \"\"\"\n",
        "    predicted_y = []\n",
        "    for each_document in test_x:\n",
        "        predicted_y.append(trained_classifier.predict(np.reshape(each_document, (1, -1))))\n",
        "    return predicted_y, test_y\n",
        "\n",
        "\n",
        "def convert_data_for_SGD_classifier(data, words_list):\n",
        "    train_x = []\n",
        "    train_y = []\n",
        "    for each_document in data:\n",
        "        train_x_for_this_document = []\n",
        "        train_y.append(each_document[\"this_is_the_class_of_the_document\"])\n",
        "        for each_word in words_list:\n",
        "            # We are using a try catch here since it may happen that the given word is not in the document\n",
        "            try:\n",
        "                train_x_for_this_document.append(each_document[each_word])\n",
        "            except:\n",
        "                # If the word is not in the test set then we just 0 as the input for the given word.\n",
        "                train_x_for_this_document.append(0)\n",
        "        train_x.append(train_x_for_this_document)\n",
        "    return train_x, train_y\n",
        "\n",
        "\n",
        "def get_data_from_given_model(spam_mail_model, ham_mail_model):\n",
        "    \"\"\"\n",
        "    This is the function used to divide the data into test and train data\n",
        "    :param spam_mail_model: This is the representation(list) of each spam document in the given format\n",
        "    :param ham_mail_model: This is the representation(list) of each ham document in the given format\n",
        "    :return: the train and test set\n",
        "    \"\"\"\n",
        "    for each_dict in spam_mail_model:\n",
        "        each_dict[\"this_is_the_class_of_the_document\"] = 1\n",
        "    for each_dict in ham_mail_model:\n",
        "        each_dict[\"this_is_the_class_of_the_document\"] = 0\n",
        "    all_data = spam_mail_model + ham_mail_model\n",
        "    # We are using this step to shuffle our data so that different data goes into training and testing everything\n",
        "    return all_data\n",
        "\n",
        "\n",
        "def divide_into_validation_and_train(spam_mail_model, ham_mail_model):\n",
        "    \"\"\"\n",
        "    This is the function used to divide the data into test and train data\n",
        "    :param spam_mail_model: This is the representation(list) of each spam document in the given format\n",
        "    :param ham_mail_model: This is the representation(list) of each ham document in the given format\n",
        "    :return: the train and test set\n",
        "    \"\"\"\n",
        "    # Here spam is 1 and ham is 0 (since we are using sigmoid)\n",
        "    for each_dict in spam_mail_model:\n",
        "        each_dict[\"this_is_the_class_of_the_document\"] = 1\n",
        "    for each_dict in ham_mail_model:\n",
        "        each_dict[\"this_is_the_class_of_the_document\"] = 0\n",
        "    all_data = spam_mail_model + ham_mail_model\n",
        "    # We are using this step to shuffle our data so that different data goes into training and testing everything\n",
        "    random.shuffle(all_data)\n",
        "    train_data = all_data[0: int(len(all_data) * .70)]\n",
        "    validation_data = all_data[int(len(all_data) * .70): -1]\n",
        "    return train_data, validation_data"
      ],
      "metadata": {
        "id": "4BXjGy1ebQ5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def evaluate_SGD_bag_of_words(dataset_name):\n",
        "    \"\"\"\n",
        "    This is the method used for evaluation of multinomial NB on a particular dataset\n",
        "    :param dataset_name: This is the given dataset name\n",
        "    :return: All the evaluation metrics\n",
        "    \"\"\"\n",
        "    # We first import training data for the training\n",
        "    try:\n",
        "        spam_email_bag_of_words1, ham_email_bag_of_words1, text_frequency_in_all_folders, spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary = bag_of_words_model(\n",
        "            dataset_name, True)\n",
        "        spam_email_bag_of_words_test, ham_email_bag_of_words_test, text_frequency_in_all_folders_test, spam_mail_in_all_documents_test, ham_mail_in_all_documents_test, size_of_whole_dataset_test, size_of_spam_dataset_test, size_of_ham_dataset_test, total_file_dictionary_test = bag_of_words_model(\n",
        "            dataset_name, False)\n",
        "    except:\n",
        "        print( \"You have given wrong file name, please check and run again\")\n",
        "        exit(-1)\n",
        "    train_data, validation_data = divide_into_validation_and_train(spam_email_bag_of_words1,\n",
        "                                                                                 ham_email_bag_of_words1)\n",
        "    test_data = get_data_from_given_model(spam_email_bag_of_words_test, ham_email_bag_of_words_test)\n",
        "    words_list = list(train_data[0])\n",
        "    # we import the train, test and validation datasets\n",
        "    train_x, train_y = convert_data_for_SGD_classifier(train_data, words_list)\n",
        "    test_x, test_y = convert_data_for_SGD_classifier(test_data, words_list)\n",
        "    valid_x, valid_y = convert_data_for_SGD_classifier(validation_data, words_list)\n",
        "    # In this step we are getting the best parameters for the sklearn SGD classifier\n",
        "    classifier_model = parameter_tuning(valid_x, valid_y)\n",
        "    # In this step the classifier model is being trained on the training dataset\n",
        "    trained_classifier_model = train_SGD(train_x, train_y, classifier_model)\n",
        "    # In this step we find the output for the classifier.\n",
        "    predicted_y, actual_y = test_SGD(trained_classifier_model, test_x, test_y)\n",
        "    # Now calculate the evaluation metrics\n",
        "    \n",
        "    accur = accuracy(actual_y, predicted_y)\n",
        "    prec = precision(actual_y, predicted_y)\n",
        "    rec = recall(actual_y, predicted_y)\n",
        "    f1 = f1_score(rec, prec)\n",
        "    return accur, prec, rec, f1\n",
        "\n",
        "\n",
        "def evaluate_SGD_bernoulli_model(dataset_name):\n",
        "    \"\"\"\n",
        "    This is the method used for evaluation of multinomial NB on a particular dataset\n",
        "    :param dataset_name: This is the given dataset name\n",
        "    :return: All the evaluation metrics\n",
        "    \"\"\"\n",
        "    # We first import training data for the training\n",
        "    try:\n",
        "        spam_email_bernoulli_model1, ham_email_bernoulli_model1, spam_mail_in_all_documents, ham_mail_in_all_documents, size_of_whole_dataset, size_of_spam_dataset, size_of_ham_dataset, total_file_dictionary = convert_to_bernoulli_model(\n",
        "            dataset_name, True)\n",
        "        spam_email_bernoulli_model_test, ham_email_bernoulli_model_test, spam_mail_in_all_documents_test, ham_mail_in_all_documents_test, size_of_whole_dataset_test, size_of_spam_dataset_test, size_of_ham_dataset_test, total_file_dictionary_test = convert_to_bernoulli_model(\n",
        "            dataset_name, False)\n",
        "    except:\n",
        "        print (\"You have given wrong file name, please check and run again\")\n",
        "        exit(-1)\n",
        "    train_data, validation_data = divide_into_validation_and_train(spam_email_bernoulli_model1,\n",
        "                                                                                 ham_email_bernoulli_model1)\n",
        "    test_data = get_data_from_given_model(spam_email_bernoulli_model_test, ham_email_bernoulli_model_test)\n",
        "    words_list = list(train_data[0])\n",
        "    # we import the train, test and validation datasets\n",
        "    train_x, train_y = convert_data_for_SGD_classifier(train_data, words_list)\n",
        "    test_x, test_y = convert_data_for_SGD_classifier(test_data, words_list)\n",
        "    valid_x, valid_y = convert_data_for_SGD_classifier(validation_data, words_list)\n",
        "    # In this step we are getting the best parameters for the sklearn SGD classifier\n",
        "    classifier_model = parameter_tuning(valid_x, valid_y)\n",
        "    # In this step the classifier model is being trained on the training dataset\n",
        "    trained_classifier_model = train_SGD(train_x, train_y, classifier_model)\n",
        "    # In this step we find the output for the classifier.\n",
        "    predicted_y, actual_y = test_SGD(trained_classifier_model, test_x, test_y)\n",
        "    # Now calculate the evaluation metrics\n",
        "    accur = accuracy(actual_y, predicted_y)\n",
        "    prec = precision(actual_y, predicted_y)\n",
        "    rec = recall(actual_y, predicted_y)\n",
        "    f1 = f1_score(rec, prec)\n",
        "    return accur, prec, rec, f1\n",
        "\n",
        "# evaluate_SGD_bag_of_words(dataset_name) # for bow\n",
        "# evaluate_SGD_bernoulli_model(dataset_name) # for bm"
      ],
      "metadata": {
        "id": "C35VhuVwbk4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_SGD_bag_of_words('/content/enron1')\n"
      ],
      "metadata": {
        "id": "dJPOlCXwcDyj",
        "outputId": "e80dddc4-8136-48a5-ee36-348edbd8eb30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9912280701754386, 0.9738562091503268, 1.0, 0.9867549668874172)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_SGD_bag_of_words('/content/enron1')"
      ],
      "metadata": {
        "id": "wZp09WdpcHtT",
        "outputId": "1b48c0e0-2e62-442f-c97b-4e76a565bb14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9912280701754386, 0.9738562091503268, 1.0, 0.9867549668874172)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    }
  ]
}